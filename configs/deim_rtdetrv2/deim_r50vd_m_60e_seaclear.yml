__include__: [
  './rtdetrv2_r50vd_6x_seaclear.yml',
  '../base/rt_deim.yml',
]

output_dir: ./outputs/deim_rtdetrv2_r50vd_60e_seaclear

optimizer:
  type: AdamW
  params:
    - params: '^(?=.*backbone)(?!.*norm).*$'
      lr: 0.00002
    - params: '^(?=.*(?:norm|bn)).*$'
      weight_decay: 0.
  lr: 0.0002
  betas: [0.9, 0.999]
  weight_decay: 0.0001

# DEIM training schedule
epoches: 150         # total epochs
flat_epoch: 34       # learning rate stays constant for first 34 epochs
no_aug_epoch: 2      # last 2 epochs without augmentation

early_stopping:
  metric: 'mAP'      # validation metric to monitor
  patience: 10       # stop if no improvement for 10 epochs

train_dataloader:
  dataset:
    transforms:
      policy:
        epoch: [4, 34, 58]   # augmentation schedule
  collate_fn:
    mixup_epochs: [4, 34]
    stop_epoch: 58           # end of augmentation stage